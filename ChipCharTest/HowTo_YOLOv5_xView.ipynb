{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chip Characterization Test\n",
    "## YOLOv5 on modified MSTAR Dataset\n",
    "\n",
    "21 Aug 2022, Alex Denton, Thesis Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Before You Start \n",
    "YOLOv5 Tutorial: https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/ <br>\n",
    "My GitHub Repo: https://github.com/awd86/yolov5\n",
    "\n",
    "Notes on scripts and syntax:\n",
    "\n",
    "- I recommend doing <i>all of the following</i> on the DGX because the dataset is very large. It takes nearly an hour to transfer (once coupon'd) via USB3.0 on the DGX machines.\n",
    "\n",
    "- I did <i>not</i> use Jupyter Notebook for this task. I used PyCharm IDE to write my Python code and exectuted <i>train.py</i> from terminal. Some of the required packages are only available on Pip. You can probably use Conda and Jupyter Notebooks, but I haven't done testing. \n",
    "\n",
    "- \"!\" means that Jupyter (or a .py file) will run that command in a new terminal instance. The terminal instance will be created within your virtual environment, but each new \"!\" is a new terminal. If you need to run multiple commands in one instance, put them on 1 line with \";\" separators.\n",
    "\n",
    "- \"python\" vs. \"python3\" depends on your machine's aliasing. If you want to alias \"python\" to run \"python3\" instead of your machine's default python release, you can look up how to edit your profile. <i>Do this at your own risk!</i> I have set mine up this way and tend to write \"python\"...you can safely replace that with \"python3\" if you're having issues. \n",
    "\n",
    "### (needed before making venv and launching jupyter notebook)\n",
    "\n",
    "Clone YOLOv5 GitHub repo and install requirements.txt in a Python>=3.6.0 environment, including PyTorch>=1.7. Models and datasets download automatically from the latest YOLOv5 release. I'm sucessfully using 3.8 and 3.9 on different machines.<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "git clone https://github.com/ultralytics/yolov5\n",
    "cd yolov5\n",
    "pip install -r requirements.txt<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "NOTE: PyTorch>=1.9 with new torch.distributed.run is recommended (replaces older torch.distributed.launch commands below). See https://pytorch.org/docs/stable/distributed.html for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strike> You'll want to download my .py files into the cloned YOLOv5 Repo (to have to most current version of YOLOv5). Here are the ones you'll need:\n",
    "    \n",
    "- HowTo_YOLOv5_xView.ipynb (this document)\n",
    "- coupons.py  (divides the picture and labels into coupons)\n",
    "- split_set.py  (to divide images/labels into 'train' and 'val' sets)\n",
    "- re_classify.py  (to change class names and remove 'None' class)\n",
    "- vague_classes.py  (where new class names are specified\n",
    "- autosplit_txt.py  (replicates the autosplit files created by the YOLO converted, described below)\n",
    "\n",
    "<strike> The last piece to the puzzle is the file that converts xView labels into YOLO format. That can be found in the /data/ directory (folder) of the YOLOv5 clone'd repo. There is a file in there called 'xView.yaml' that specifies the data structure. At the bottom they've included the python code to convert your xView dataset (awesome of them!). You'll have to copy/paste to a new .py and modify the paths to fit your file structure. You're also going to end up modifying copies of this .yaml to specify the data structure for your runs.</strike>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't much prep work before training. The dataset creation routine takes care of almost everything. Just copy the synthetic datasets into the same directory as this file. Change the absolute paths in the <i>data.yaml</i> files to reflect there actual location.<br> Then ensure you have these other supporting files:\n",
    "\n",
    "- train.py\n",
    "- val.py\n",
    "- split_set.py\n",
    "- /models/ChipCT.yaml  (determines the architecture)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the local Cuda version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2018 NVIDIA Corporation\n",
      "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
      "Cuda compilation tools, release 10.0, V10.0.130\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check PyTorch version & GPU Compatability\n",
    "- torch >= 1.9\n",
    "- CudaDeviceProperties should have something under 'name' - this means it is compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.10.1+cu102 _CudaDeviceProperties(name='Tesla V100-DGXS-32GB', major=7, minor=0, total_memory=32485MB, multi_processor_count=80)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Multi-GPU DistributedDataParallel Mode\n",
    "https://github.com/ultralytics/yolov5/issues/475"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before specifying GPUs, <a href=\"https://hsf-training.github.io/hsf-training-ml-gpu-webpage/02-whichgpu/index.html\">determine the parameters</a>:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__CUDNN VERSION: 7605\n",
      "__Number CUDA Devices: 4\n",
      "__CUDA Device Name: Tesla V100-DGXS-32GB\n",
      "__CUDA Device Total Memory [GB]: 34.063712256\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "    print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strike> Note: DGX1 and DGX4 both report \"Number of CUDA Devices: 5\" but the correct number to specify is \"4\"\n",
    "<br>There are only 4 GPUs. This script might be counting the CPU as an additional CUDA device,<i> but train.py won't run</i> if you say \"5\"</strike>\n",
    "\n",
    "You will have to pass python the following along with the usual arguments:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python3 -m torch.distributed.run --nproc_per_node 4 train.py --img 512 --batch 64 --epochs 200 --data ./data_xView/xView.yaml --cfg ./models/xView_yolov5x.yaml --weights '' --name xView_coupons_yolov5x_results  --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--nproc_per_node specifies how many GPUs you would like to use. In the example above, it is 4.<br>\n",
    "\n",
    "--batch is the total batch-size. It will be divided evenly to each GPU. In the example above, it is 64/4=16 per GPU.<br>\n",
    "\n",
    "The code above will use GPUs 0... (N-1).\n",
    "\n",
    "Notes<br>\n",
    "- Windows support is untested, Linux is recommended.\n",
    "- '--batch' must be a multiple of the number of GPUs.\n",
    "- GPU 0 will take slightly more memory than the other GPUs as it maintains EMA and is responsible for checkpointing etc.\n",
    "\n",
    "If you get RuntimeError: Address already in use, it could be because you are running multiple trainings at a time. To fix this, simply use a different port number by adding --master_port like below,"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python -m torch.distributed.launch --master_port 1234 --nproc_per_node 2 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Dataset Folder Structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The folder architecture is very important!!<br>\n",
    "It doesn't have to be exactly like this, but it does need to be specified in data.yaml<br>\n",
    "\n",
    "yolov5 (contains all py code)<br>\n",
    "|_ venv()<br>\n",
    "|<br>\n",
    "|_ ChipCharTest<br>\n",
    "&emsp;   |_ models<br>\n",
    "&emsp;   | &emsp;|_ ChipCT.yaml. * model specification<br>\n",
    "&emsp;   |<br>\n",
    "&emsp;   |_ dataset_A<br>\n",
    "&emsp;   &emsp;   |_ data.yaml * class and image/label location specification<br>\n",
    "&emsp;   &emsp;   |<br>\n",
    "&emsp;   &emsp;   |_ train<br>\n",
    "&emsp;   &emsp;   | &emsp;|_ images()<br>\n",
    "&emsp;   &emsp;   | &emsp;|_ labels()<br>\n",
    "&emsp;   &emsp;   |<br>\n",
    "&emsp;   &emsp;   |_ val<br>\n",
    "&emsp;   &emsp;   | &emsp;|_ images()<br>\n",
    "&emsp;   &emsp;   | &emsp;|_ labels()<br>\n",
    "&emsp;   &emsp;   |<br>\n",
    "&emsp;   &emsp;   |_ test<br>\n",
    "&emsp;   &emsp;    &emsp;|_ images()<br>\n",
    "&emsp;   &emsp;    &emsp;|_ labels()<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script <i>split_set.py</i> should take care of this for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr border-top: 24px solid #bbb; border-radius: 10px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  * * * Prepare Dataset * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split 'train' dataset into 'train' and 'test'\n",
    "The <i>train</i> and <i>validate</i> sets have labels, but <i>test</i> does not. (well, it does...but they will be ignored)\n",
    "\n",
    "I'm rebuilding the <i>split_set.py</i> file to fully develope the train/test/validate folders based on information from:<br>\n",
    "https://www.v7labs.com/blog/train-validation-test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Set1_M35_2s1_M1/chips_10_black move is complete\n",
      "The Set1_M35_2s1_M1/chips_05_clutter move is complete\n",
      "The Set1_M35_2s1_M1/chips_10_clutter move is complete\n",
      "The Set1_M35_2s1_M1/chips_05_black move is complete\n",
      "The Set1_M35_2s1_M1/chips_20_clutter move is complete\n",
      "The Set1_M35_2s1_M1/chips_20_black move is complete\n",
      "The Set1_M35_2s1_M1/chips_20_noise move is complete\n",
      "The Set1_M35_2s1_M1/chips_05_noise move is complete\n",
      "The Set1_M35_2s1_M1/standardized move is complete\n",
      "The Set1_M35_2s1_M1/chips_10_noise move is complete\n"
     ]
    }
   ],
   "source": [
    "from split_set import split_set\n",
    "import os\n",
    "\n",
    "dataset_dir = 'Set1_M35_2s1_M1'\n",
    "for dataset in os.listdir(path=dataset_dir):\n",
    "    src_dir = '/'.join((dataset_dir,dataset))\n",
    "    split_set(src_dir,[0.7,0.15,0.15],'rand') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model configuration and architecture (needed in runtime):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check 'data.yaml' and 'xView.yaml' (model) files for each run:<br>\n",
    "- data.yaml : structure of image data folders, number of classes, name of classes\n",
    "- xView.yaml : repeat number of classes (must match), specifies architecture model in PyTorch format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an account on WandB.ai to watch training progress and get auto-generated charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wandb.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr border-top: 24px solid #bbb; border-radius: 10px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  * * * Execution * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute train.py\n",
    "\n",
    "Train Custom YOLOv5 Detector!\n",
    "Here, we are able to pass a number of arguments:<br>\n",
    "\n",
    "img: define input image size (must be <b>multiple of 32</b>)<br>\n",
    "  '--rect' allows non-square input images<br>\n",
    "batch: determine batch size (multiple of number of GPUs)<br>\n",
    "epochs: define the number of training epochs.<br>\n",
    "data: <b>set the path to our data.yaml file</b><br>\n",
    "cfg: <b>specify our model configuration xView.yaml</b><br>\n",
    "weights: specify a path to pretrained weights if using transfer learning. (Note: some available from Ultralytics)<br>\n",
    "name: <b>result names</b><br>\n",
    "nosave: only saves the final checkpoint <b>(not recommended, will keep best model if this is left out)</b><br>\n",
    "cache: cache images for faster training<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Viability of ChipCT.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m torch.distributed.run --nproc_per_node 4 ~/PycharmProjects/yolov5/train.py --img 78 --batch 64 --epochs 20 --data ./Set1_M35_2s1_M1/chips_05_black/data.yaml --cfg ./models/ChipCT.yaml --weights '' --name ChipCT_viability_1  --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***IMPORTANT NOTE*** Do <i>not</i> run these sequentially. You must change the name of your label folder. Otherwise you will simply retrain on the previous labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chip Characterization Test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "!python3 -m torch.distributed.run --nproc_per_node 4 ~/PycharmProjects/yolov5/train.py --img 78 --batch 64 --epochs 20 --data ./Set1_M35_2s1_M1/chips_05_black/data.yaml --cfg ./models/ChipCT.yaml --weights '' --name ChipCT_viability_1  --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***IMPORTANT NOTE*** Do <i>not</i> run these sequentially. You must change the name of your label folder. Otherwise you will simply retrain on the previous labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
