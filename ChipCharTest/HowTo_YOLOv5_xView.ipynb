{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chip Characterization Test\n",
    "## YOLOv5 on modified MSTAR Dataset\n",
    "\n",
    "21 Aug 2022, Alex Denton, Thesis Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Before You Start \n",
    "YOLOv5 Tutorial: https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/ <br>\n",
    "My GitHub Repo: https://github.com/awd86/yolov5\n",
    "\n",
    "Notes on scripts and syntax:\n",
    "\n",
    "- I recommend doing <i>all of the following</i> on the DGX because the dataset is very large. It takes nearly an hour to transfer (once coupon'd) via USB3.0 on the DGX machines.\n",
    "\n",
    "- I did <i>not</i> use Jupyter Notebook for this task. I used PyCharm IDE to write my Python code and exectuted <i>train.py</i> from terminal. Some of the required packages are only available on Pip. You can probably use Conda and Jupyter Notebooks, but I haven't done testing. \n",
    "\n",
    "- \"!\" means that Jupyter (or a .py file) will run that command in a new terminal instance. The terminal instance will be created within your virtual environment, but each new \"!\" is a new terminal. If you need to run multiple commands in one instance, put them on 1 line with \";\" separators.\n",
    "\n",
    "- \"python\" vs. \"python3\" depends on your machine's aliasing. If you want to alias \"python\" to run \"python3\" instead of your machine's default python release, you can look up how to edit your profile. <i>Do this at your own risk!</i> I have set mine up this way and tend to write \"python\"...you can safely replace that with \"python3\" if you're having issues. \n",
    "\n",
    "### (needed before making venv and launching jupyter notebook)\n",
    "\n",
    "Clone YOLOv5 GitHub repo and install requirements.txt in a Python>=3.6.0 environment, including PyTorch>=1.7. Models and datasets download automatically from the latest YOLOv5 release. I'm sucessfully using 3.8 and 3.9 on different machines.<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "git clone https://github.com/ultralytics/yolov5\n",
    "cd yolov5\n",
    "pip install -r requirements.txt<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "NOTE: PyTorch>=1.9 with new torch.distributed.run is recommended (replaces older torch.distributed.launch commands below). See https://pytorch.org/docs/stable/distributed.html for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strike> You'll want to download my .py files into the cloned YOLOv5 Repo (to have to most current version of YOLOv5). Here are the ones you'll need:\n",
    "    \n",
    "- HowTo_YOLOv5_xView.ipynb (this document)\n",
    "- coupons.py  (divides the picture and labels into coupons)\n",
    "- split_set.py  (to divide images/labels into 'train' and 'val' sets)\n",
    "- re_classify.py  (to change class names and remove 'None' class)\n",
    "- vague_classes.py  (where new class names are specified\n",
    "- autosplit_txt.py  (replicates the autosplit files created by the YOLO converted, described below)\n",
    "\n",
    "<strike> The last piece to the puzzle is the file that converts xView labels into YOLO format. That can be found in the /data/ directory (folder) of the YOLOv5 clone'd repo. There is a file in there called 'xView.yaml' that specifies the data structure. At the bottom they've included the python code to convert your xView dataset (awesome of them!). You'll have to copy/paste to a new .py and modify the paths to fit your file structure. You're also going to end up modifying copies of this .yaml to specify the data structure for your runs.</strike>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't much prep work before training. The dataset creation routine takes care of almost everything. Just copy the synthetic datasets into the same directory as this file. Change the absolute paths in the <i>data.yaml</i> files to reflect there actual location.<br> Then ensure you have these other supporting files:\n",
    "\n",
    "- train.py\n",
    "- val.py\n",
    "- split_set.py\n",
    "- /models/ChipCT.yaml  (determines the architecture)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the local Cuda version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2018 NVIDIA Corporation\n",
      "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
      "Cuda compilation tools, release 10.0, V10.0.130\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check PyTorch version & GPU Compatability\n",
    "- torch >= 1.9\n",
    "- CudaDeviceProperties should have something under 'name' - this means it is compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.10.1+cu102 _CudaDeviceProperties(name='Tesla V100-DGXS-32GB', major=7, minor=0, total_memory=32485MB, multi_processor_count=80)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Multi-GPU DistributedDataParallel Mode\n",
    "https://github.com/ultralytics/yolov5/issues/475"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before specifying GPUs, <a href=\"https://hsf-training.github.io/hsf-training-ml-gpu-webpage/02-whichgpu/index.html\">determine the parameters</a>:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__CUDNN VERSION: 7605\n",
      "__Number CUDA Devices: 4\n",
      "__CUDA Device Name: Tesla V100-DGXS-32GB\n",
      "__CUDA Device Total Memory [GB]: 34.063712256\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "    print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strike> Note: DGX1 and DGX4 both report \"Number of CUDA Devices: 5\" but the correct number to specify is \"4\"\n",
    "<br>There are only 4 GPUs. This script might be counting the CPU as an additional CUDA device,<i> but train.py won't run</i> if you say \"5\"</strike>\n",
    "\n",
    "You will have to pass python the following along with the usual arguments:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python3 -m torch.distributed.run --nproc_per_node 4 train.py --img 512 --batch 64 --epochs 200 --data ./data_xView/xView.yaml --cfg ./models/xView_yolov5x.yaml --weights '' --name xView_coupons_yolov5x_results  --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--nproc_per_node specifies how many GPUs you would like to use. In the example above, it is 4.<br>\n",
    "\n",
    "--batch is the total batch-size. It will be divided evenly to each GPU. In the example above, it is 64/4=16 per GPU.<br>\n",
    "\n",
    "The code above will use GPUs 0... (N-1).\n",
    "\n",
    "Notes<br>\n",
    "- Windows support is untested, Linux is recommended.\n",
    "- '--batch' must be a multiple of the number of GPUs.\n",
    "- GPU 0 will take slightly more memory than the other GPUs as it maintains EMA and is responsible for checkpointing etc.\n",
    "\n",
    "If you get RuntimeError: Address already in use, it could be because you are running multiple trainings at a time. To fix this, simply use a different port number by adding --master_port like below,"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python -m torch.distributed.launch --master_port 1234 --nproc_per_node 2 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Dataset Folder Structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The folder architecture is very important!!<br>\n",
    "It doesn't have to be exactly like this, but it does need to be specified in data.yaml<br>\n",
    "\n",
    "yolov5 (contains all py code)<br>\n",
    "|_ venv()<br>\n",
    "|<br>\n",
    "|_ ChipCharTest<br>\n",
    "&emsp;   |_ models<br>\n",
    "&emsp;   | &emsp;|_ ChipCT.yaml. * model specification<br>\n",
    "&emsp;   |<br>\n",
    "&emsp;   |_ dataset_A<br>\n",
    "&emsp;   &emsp;   |_ data.yaml * class and image/label location specification<br>\n",
    "&emsp;   &emsp;   |<br>\n",
    "&emsp;   &emsp;   |_ train<br>\n",
    "&emsp;   &emsp;   | &emsp;|_ images()<br>\n",
    "&emsp;   &emsp;   | &emsp;|_ labels()<br>\n",
    "&emsp;   &emsp;   |<br>\n",
    "&emsp;   &emsp;   |_ val<br>\n",
    "&emsp;   &emsp;   | &emsp;|_ images()<br>\n",
    "&emsp;   &emsp;   | &emsp;|_ labels()<br>\n",
    "&emsp;   &emsp;   |<br>\n",
    "&emsp;   &emsp;   |_ test<br>\n",
    "&emsp;   &emsp;    &emsp;|_ images()<br>\n",
    "&emsp;   &emsp;    &emsp;|_ labels()<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script <i>split_set.py</i> should take care of this for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr border-top: 24px solid #bbb; border-radius: 10px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  * * * Prepare Dataset * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split 'train' dataset into 'train' and 'test'\n",
    "The <i>train</i> and <i>validate</i> sets have labels, but <i>test</i> does not. (well, it does...but they will be ignored)\n",
    "\n",
    "I'm rebuilding the <i>split_set.py</i> file to fully develope the train/test/validate folders based on information from:<br>\n",
    "https://www.v7labs.com/blog/train-validation-test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Set1_M35_2s1_M1/chips_10_black move is complete\n",
      "The Set1_M35_2s1_M1/chips_05_clutter move is complete\n",
      "The Set1_M35_2s1_M1/chips_10_clutter move is complete\n",
      "The Set1_M35_2s1_M1/chips_05_black move is complete\n",
      "The Set1_M35_2s1_M1/chips_20_clutter move is complete\n",
      "The Set1_M35_2s1_M1/chips_20_black move is complete\n",
      "The Set1_M35_2s1_M1/chips_20_noise move is complete\n",
      "The Set1_M35_2s1_M1/chips_05_noise move is complete\n",
      "The Set1_M35_2s1_M1/standardized move is complete\n",
      "The Set1_M35_2s1_M1/chips_10_noise move is complete\n"
     ]
    }
   ],
   "source": [
    "from split_set import split_set\n",
    "import os\n",
    "\n",
    "dataset_dir = 'Set1_M35_2s1_M1'\n",
    "for dataset in os.listdir(path=dataset_dir):\n",
    "    src_dir = '/'.join((dataset_dir,dataset))\n",
    "    split_set(src_dir,[0.7,0.15,0.15],'rand') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model configuration and architecture (needed in runtime):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check 'data.yaml' and 'xView.yaml' (model) files for each run:<br>\n",
    "- data.yaml : structure of image data folders, number of classes, name of classes\n",
    "- xView.yaml : repeat number of classes (must match), specifies architecture model in PyTorch format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an account on WandB.ai to watch training progress and get auto-generated charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wandb.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr border-top: 24px solid #bbb; border-radius: 10px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  * * * Execution * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute train.py\n",
    "\n",
    "Train Custom YOLOv5 Detector!\n",
    "Here, we are able to pass a number of arguments:<br>\n",
    "\n",
    "img: define input image size (must be <b>multiple of 32</b>)<br>\n",
    "  '--rect' allows non-square input images<br>\n",
    "batch: determine batch size (multiple of number of GPUs)<br>\n",
    "epochs: define the number of training epochs.<br>\n",
    "data: <b>set the path to our data.yaml file</b><br>\n",
    "cfg: <b>specify our model configuration xView.yaml</b><br>\n",
    "weights: specify a path to pretrained weights if using transfer learning. (Note: some available from Ultralytics)<br>\n",
    "name: <b>result names</b><br>\n",
    "nosave: only saves the final checkpoint <b>(not recommended, will keep best model if this is left out)</b><br>\n",
    "cache: cache images for faster training<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Viability of ChipCT.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mawd86\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=, cfg=./models/ChipCT.yaml, data=./Set1_M35_2s1_M1/chips_05_black/data.yaml, hyp=../data/hyps/hyp.scratch.yaml, epochs=20, batch_size=64, imgsz=78, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=../runs/train, name=ChipCT_viability_1, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/awd86/yolov5 ‚úÖ\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m scikit-learn==1.0.1 not found and is required by YOLOv5, attempting auto-update...\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement scikit-learn==1.0.1 (from versions: 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.14, 0.14.1, 0.15.0b1, 0.15.0b2, 0.15.0, 0.15.1, 0.15.2, 0.16b1, 0.16.0, 0.16.1, 0.17b1, 0.17, 0.17.1, 0.18, 0.18.1, 0.18.2, 0.19b2, 0.19.0, 0.19.1, 0.19.2, 0.20rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.20.4, 0.21rc2, 0.21.0, 0.21.1, 0.21.2, 0.21.3, 0.22rc2.post1, 0.22rc3, 0.22, 0.22.1, 0.22.2, 0.22.2.post1, 0.23.0rc1, 0.23.0, 0.23.1, 0.23.2, 0.24.dev0, 0.24.0rc1, 0.24.0, 0.24.1, 0.24.2)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for scikit-learn==1.0.1\u001b[0m\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Command 'pip install 'scikit-learn==1.0.1'' returned non-zero exit status 1.\n",
      "YOLOv5 üöÄ v6.0-122-ga2a7d3a torch 1.10.1+cu102 CUDA:0 (Tesla V100-DGXS-32GB, 32486MiB)\n",
      "\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir ../runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/chacha_thesis/PycharmProjects/yolov5/ChipCharTest/wandb/run-20220823_200205-fjxlxlor\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mChipCT_viability_1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/awd86/train\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/awd86/train/runs/fjxlxlor\u001b[0m\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      8800  models.common.Conv                      [3, 80, 6, 2, 2]              \n",
      "  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n",
      "  2                -1  4    309120  models.common.C3                        [160, 160, 4]                 \n",
      "  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n",
      "  4                -1  8   2259200  models.common.C3                        [320, 320, 8]                 \n",
      "  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n",
      "  6                -1 12  13125120  models.common.C3                        [640, 640, 12]                \n",
      "  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n",
      "  8                -1  4  19676160  models.common.C3                        [1280, 1280, 4]               \n",
      "  9                -1  1   4099840  models.common.SPPF                      [1280, 1280, 5]               \n",
      " 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \n",
      " 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  4   1335040  models.common.C3                        [640, 320, 4, False]          \n",
      " 18                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  4   4922880  models.common.C3                        [640, 640, 4, False]          \n",
      " 21                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \n",
      " 24      [17, 20, 23]  1    114393  models.yolo.Detect                      [12, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\n",
      "Model Summary: 567 layers, 86291833 parameters, 86291833 gradients\n",
      "\n",
      "WARNING: --img-size 78 must be multiple of max stride 32, updating to 96\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 123 weight, 126 weight (no decay), 126 bias\n",
      "WARNING: --img-size 78 must be multiple of max stride 32, updating to 96\n",
      "WARNING: --img-size 78 must be multiple of max stride 32, updating to 96\n",
      "WARNING: --img-size 78 must be multiple of max stride 32, updating to 96\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'Set1_M35_2s1_M1/chips_05_black/train/labels' images and labels.\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: Set1_M35_2s1_M1/chips_05_black/train/labels.cache\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100%|‚ñà‚ñà| 2721/2721 [00:00<00:00, 8791.90it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'Set1_M35_2s1_M1/chips_05_black/train/labels.cache' images and l\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'Set1_M35_2s1_M1/chips_05_black/train/labels.cache' images and l\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'Set1_M35_2s1_M1/chips_05_black/train/labels.cache' images and l\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100%|‚ñà‚ñà| 2721/2721 [00:00<00:00, 7717.14it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100%|‚ñà‚ñà| 2721/2721 [00:00<00:00, 7838.75it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100%|‚ñà‚ñà| 2721/2721 [00:00<00:00, 7501.02it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning 'Set1_M35_2s1_M1/chips_05_black/val/labels' images and labels...58\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: Set1_M35_2s1_M1/chips_05_black/val/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 583/583 [00:00<00:00, 7855.18it/s]\u001b[0m\n",
      "Plotting labels... \n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 3.03, Best Possible Recall (BPR) = 0.7773. Attempting to improve anchors, please wait...\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mWARNING: Extremely small objects found. 562 of 2721 labels are < 3 pixels in size.\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 2721 points...\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.68 anchors past thr\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=96, metric_all=0.416/0.871-mean/best, past_thr=0.581-mean: 3,3,  5,8,  8,14,  11,20,  19,12,  18,17,  34,14,  33,18,  30,21\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.8788: 100%|‚ñà| 1\u001b[0m\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.79 anchors past thr\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=96, metric_all=0.418/0.879-mean/best, past_thr=0.577-mean: 3,3,  5,7,  9,14,  17,12,  12,20,  18,17,  34,14,  33,18,  30,21\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
      "\n",
      "Image sizes 96 train, 96 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m../runs/train/ChipCT_viability_12\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      0/19     8.99G    0.1491  0.002819   0.06726        20        96:   2%|‚ñè  Reducer buckets have been rebuilt in this iteration.\n",
      "      0/19     9.03G    0.1408  0.003524   0.06253        18        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583    0.00314      0.293     0.0024   0.000616\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      1/19        9G    0.1344  0.003948    0.0509        11        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583    0.00122     0.0796    0.00108   0.000113\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      2/19        9G    0.1322  0.004018   0.04486        10        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583     0.0007      0.154   0.000578   7.86e-05\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      3/19        9G    0.1306  0.004045   0.04228        15        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583   0.000785      0.176   0.000642   9.34e-05\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      4/19        9G    0.1272  0.004413   0.04108        11        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583     0.0015      0.341    0.00225   0.000421\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      5/19        9G    0.1199  0.005654   0.03986        13        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583    0.00557      0.301    0.00366   0.000547\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      6/19        9G    0.1091  0.006464   0.03866         9        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583     0.0798      0.109     0.0488    0.00778\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      7/19        9G   0.09966  0.007465   0.03537        15        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583      0.102      0.214     0.0669     0.0196\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      8/19        9G   0.09233  0.007188   0.03249         9        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583     0.0542      0.113      0.025    0.00602\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      9/19        9G   0.08424  0.007571   0.02861        12        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583      0.504      0.293      0.184     0.0576\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     10/19        9G   0.08359  0.007199   0.02535        12        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583      0.123      0.321     0.0862     0.0271\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     11/19        9G   0.08224  0.007367   0.02371        10        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583     0.0389       0.46     0.0273    0.00977\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     12/19        9G   0.07901  0.007164   0.02261        14        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583      0.206      0.454      0.216     0.0609\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     13/19        9G   0.07447  0.007219   0.02093        15        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583      0.379       0.32      0.226     0.0886\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     14/19        9G   0.07387  0.007038   0.01734         8        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583      0.539       0.53      0.399      0.185\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     15/19        9G   0.07282  0.006858   0.01626        11        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583      0.543      0.535      0.402      0.186\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     16/19        9G   0.07065  0.006952   0.01375        14        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583      0.133      0.486      0.102     0.0464\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     17/19        9G    0.0694  0.006864   0.01196        13        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583      0.555      0.473      0.396      0.129\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     18/19        9G   0.06815  0.006958   0.01156         8        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583      0.591       0.53      0.413      0.195\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     19/19        9G   0.06461  0.007173   0.01119        12        96: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583      0.551      0.562       0.45      0.256\n",
      "\n",
      "20 epochs completed in 0.089 hours.\n",
      "Optimizer stripped from ../runs/train/ChipCT_viability_12/weights/last.pt, 173.1MB\n",
      "Optimizer stripped from ../runs/train/ChipCT_viability_12/weights/best.pt, 173.1MB\n",
      "\n",
      "Validating ../runs/train/ChipCT_viability_12/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model Summary: 444 layers, 86247433 parameters, 0 gradients\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        583        583      0.551      0.562       0.45      0.256\n",
      "                bmp2        583        236      0.409      0.419      0.255      0.175\n",
      "                  m2        583        182      0.627       0.49      0.428      0.263\n",
      "                 m60        583        165      0.616      0.776      0.668      0.331\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñá‚ñÉ‚ñá‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÇ‚ñÖ‚ñÜ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñá‚ñÉ‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÖ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/mAP_0.5 0.44973\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/mAP_0.5:0.95 0.25577\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision 0.55062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall 0.56171\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/box_loss 0.06461\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/cls_loss 0.01119\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/obj_loss 0.00717\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/box_loss 0.06305\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/cls_loss 0.00585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         val/obj_loss 0.00364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr0 0.00091\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr1 0.00091\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                x/lr2 0.01501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mChipCT_viability_1\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/awd86/train/runs/fjxlxlor\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 337 media file(s), 1 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220823_200205-fjxlxlor/logs\u001b[0m\n",
      "Results saved to \u001b[1m../runs/train/ChipCT_viability_12\u001b[0m\n",
      "Destroying process group... \n"
     ]
    }
   ],
   "source": [
    "!python3 -m torch.distributed.run --nproc_per_node 4 ~/PycharmProjects/yolov5/train.py --img 78 --batch 64 --epochs 20 --data ./Set1_M35_2s1_M1/chips_05_black/data.yaml --cfg ./models/ChipCT.yaml --weights '' --name ChipCT_viability_1  --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***IMPORTANT NOTE*** Do <i>not</i> run these sequentially. You must change the name of your label folder. Otherwise you will simply retrain on the previous labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chip Characterization Test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python3 -m torch.distributed.run --nproc_per_node 4 ~/PycharmProjects/yolov5/train.py --img 78 --batch 64 --epochs 20 --data ./Set1_M35_2s1_M1/chips_05_black/data.yaml --cfg ./models/ChipCT.yaml --weights '' --name ChipCT_viability_1  --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***IMPORTANT NOTE*** Do <i>not</i> run these sequentially. You must change the name of your label folder. Otherwise you will simply retrain on the previous labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
